{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import cv2\n",
    "import skvideo.io\n",
    "import imageio\n",
    "import pytube\n",
    "\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import humanize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage.measure import compare_mse, compare_ssim, compare_nrmse\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import hamming\n",
    "from imageai.Detection import ObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FPS(video_loc):\n",
    "    \"\"\"\n",
    "    A function to compute the frame per second of the video\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    video_loc: str\n",
    "        Location of the input video\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        A float number that is the FPS of the given video\n",
    "    \"\"\"\n",
    "    \n",
    "    video = cv2.VideoCapture(video_loc)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask(img, coordinates):\n",
    "    \"\"\"\n",
    "    A function to add masks to the image on given coordinates\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img: \n",
    "        Image object to add masks\n",
    "    coordinates:\n",
    "        Coordinates to add masks\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    image\n",
    "        An image object with the masks   \n",
    "    \"\"\"\n",
    "    \n",
    "    mask = np.zeros(img.shape, dtype = 'uint8')\n",
    "    \n",
    "    for coord in coordinates:\n",
    "        cv2.rectangle(mask, coord[0], coord[1], (255, 255, 255), -1);\n",
    "        \n",
    "    maskedImg = cv2.bitwise_and(img, mask)\n",
    "    return maskedImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_object(img, model='resnet50_coco_best_v2.0.1.h5', minimum_prob=80):\n",
    "    \"\"\"\n",
    "    A function to detect object from images using pre-trained network\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img:\n",
    "        Image object to detect objects\n",
    "    model:\n",
    "        The pre-trained network model (https://github.com/OlafenwaMoses/ImageAI/blob/master/imageai/Detection/README.md)\n",
    "    minimum_prob: int\n",
    "        Threshold to identify objects\n",
    "    \n",
    "    Returns\n",
    "    -------   \n",
    "        An array of dictionaries with the information of all detected objects\n",
    "    \"\"\"\n",
    "    \n",
    "    detector = ObjectDetection()\n",
    "    detector.setModelTypeAsRetinaNet()\n",
    "    # the pre-trained network should be in this folder\n",
    "    detector.setModelPath(os.path.join('/home/idies/workspace/Storage/Cong/persistent/video', model))\n",
    "    detector.loadModel()\n",
    "    detections = detector.detectObjectsFromImage(input_image=img, input_type='array', minimum_percentage_probability=minimum_prob)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram(img):\n",
    "    \"\"\"\n",
    "    A function to compute the histogram of an image\n",
    "    \"\"\"\n",
    "    \n",
    "    h, w = img.shape\n",
    "    hist = [0.0] * 256\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            hist[img[i, j]] += 1\n",
    "    return np.array(hist) / (h * w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_exposure(img):\n",
    "    \"\"\"\n",
    "    A function to normalize the exposure of an image\n",
    "    \"\"\"\n",
    "    \n",
    "    img = img.astype(int)\n",
    "    hist = get_histogram(img)\n",
    "    # get the sum of vals accumulated by each position in hist\n",
    "    cdf = np.array([sum(hist[:i+1]) for i in range(len(hist))])\n",
    "    # determine the normalization values for each unit of the cdf\n",
    "    sk = np.uint8(255 * cdf)\n",
    "    # normalize each position in the output image\n",
    "    h, w = img.shape\n",
    "    normalized = np.zeros_like(img)\n",
    "    for i in range(0, h):\n",
    "        for j in range(0, w):\n",
    "            normalized[i, j] = sk[img[i, j]]\n",
    "    return normalized.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_emd(img1, img2):\n",
    "    \"\"\"\n",
    "    A function to measure image similarity using EMD method\n",
    "    \"\"\"\n",
    "    \n",
    "    img1 = normalize_exposure(img1)\n",
    "    img2 = normalize_exposure(img2)\n",
    "    return wasserstein_distance(get_histogram(img1), get_histogram(img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_orb(img1, img2, threshold):\n",
    "    \"\"\"\n",
    "    A function to measure image similarity using ORB method\n",
    "    \"\"\"\n",
    "    \n",
    "    orb = cv2.ORB_create()\n",
    "    kp_1, desc_1 = orb.detectAndCompute(img1, None)\n",
    "    kp_2, desc_2 = orb.detectAndCompute(img2, None)\n",
    "    if desc_1 is None or desc_2 is None:\n",
    "        return 0\n",
    "    \n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(desc_1, desc_2)\n",
    "    if len(matches) == 0:\n",
    "        return 0\n",
    "    similar = [i for i in matches if i.distance < threshold]\n",
    "    return len(similar) / len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dhash(img, hashSize=8):\n",
    "    \"\"\"\n",
    "    A function to compute dHash of an image\n",
    "    \"\"\"\n",
    "    \n",
    "    # resize the input image, adding a single column (row) so we can compute the gradient\n",
    "    resized_col = cv2.resize(img, (hashSize+1,hashSize))\n",
    "    resized_row = cv2.resize(img, (hashSize,hashSize+1))\n",
    " \n",
    "    # compute the (relative) gradient between adjacent pixels\n",
    "    diff_col = resized_col[:,1:] > resized_col[:,:-1]\n",
    "    diff_row = resized_row[1:,:] > resized_row[:-1,:]\n",
    "\n",
    "    # convert the difference image to a hash\n",
    "    return sum([2**i for (i, v) in enumerate(np.append(diff_col.flatten(), diff_row.flatten())) if v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dhash(img1, img2, hashSize=8):\n",
    "    \"\"\"\n",
    "    A function to measure image similarity using dHash method\n",
    "    \"\"\"\n",
    "    \n",
    "    h1 = [int(d) for d in str(int(dhash(img1)))]\n",
    "    h2 = [int(d) for d in str(int(dhash(img2)))]\n",
    "    \n",
    "    if len(h1) == len(h2):\n",
    "        dHash = hamming(h1, h2)\n",
    "    else:\n",
    "        dHash = 1\n",
    "    return dHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_phash(img1, img2):\n",
    "    \"\"\"\n",
    "    A function to measure image similarity using pHash method\n",
    "    \"\"\"\n",
    "    \n",
    "    phash = cv2.img_hash_PHash.create()\n",
    "    return phash.compare(phash.compute(img1), phash.compute(img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_image(image_df, method, ascending, idx, start, end, coords):\n",
    "    \"\"\"\n",
    "    A function to find the similar image of a given image\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_df: \n",
    "        A dataframe with the location of images\n",
    "    method:\n",
    "        Methods used to measure image similarity, could be MSE, EMD, ORB, dHash, pHash\n",
    "    ascending:\n",
    "        Whether the metric used is ranked in ascending oreder\n",
    "    idx:\n",
    "        Target image index\n",
    "    start:\n",
    "        Start index of the image to search\n",
    "    end:\n",
    "        End index of the image to search\n",
    "    coords:\n",
    "        Coordinates to add mask\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        A dataframe with all the similar images (index/location)\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {}\n",
    "            \n",
    "    for i in range(start, min(end, image_df.shape[0])):\n",
    "        \n",
    "        img = imread(image_df['img_path'][idx])\n",
    "        \n",
    "        temp_img = imread(image_df['img_path'][i])\n",
    "        \n",
    "        if method == 'EMD' or method == 'ORB' or method == 'dHash':\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            temp_img = cv2.cvtColor(temp_img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        img = add_mask(img, coords)\n",
    "        temp_img = add_mask(temp_img, coords)\n",
    "        \n",
    "        detections = detect_object(img)\n",
    "        carcoords = []\n",
    "        for detection in detections:\n",
    "            if detection.get('name') == 'car':\n",
    "                temp = detection.get('box_points')\n",
    "                carcoords.append(temp)\n",
    "        \n",
    "        tempdetections = detect_object(temp_img)\n",
    "        for detection in tempdetections:\n",
    "            if detection.get('name') == 'car':\n",
    "                temp = detection.get('box_points')\n",
    "                carcoords.append(temp)\n",
    "        \n",
    "        for carcoord in carcoords:\n",
    "            cv2.rectangle(img,(carcoord[0],carcoord[1]),(carcoord[2],carcoord[3]),(0,0,0),-1);\n",
    "            cv2.rectangle(temp_img,(carcoord[0],carcoord[1]),(carcoord[2],carcoord[3]),(0,0,0),-1);\n",
    "        \n",
    "        if method == 'MSE':\n",
    "            results_dict[i] = [image_df['img_path'][i], image_df['img_index'][i], compare_mse(img, temp_img)]\n",
    "        elif method == 'EMD':\n",
    "            results_dict[i] = [image_df['img_path'][i], image_df['img_index'][i], compare_emd(img, temp_img)]\n",
    "        elif method == 'ORB':\n",
    "            results_dict[i] = [image_df['img_path'][i], image_df['img_index'][i], compare_orb(img, temp_img, threshold)]\n",
    "        elif method == 'dHash':\n",
    "            results_dict[i] = [image_df['img_path'][i], image_df['img_index'][i], compare_dhash(img, temp_img)]\n",
    "        elif method == 'pHash':\n",
    "            results_dict[i] = [image_df['img_path'][i], image_df['img_index'][i], compare_phash(img, temp_img)]\n",
    "        else:\n",
    "            results_dict[i] = [image_df['img_path'][i], image_df['img_index'][i], compare_ssim(img, temp_img, multichannel=True)]\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "    results_df.columns = ['img_path', 'img_index', method]\n",
    "    results_df = results_df.sort_values(by=method, ascending=ascending)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/idies/workspace/Storage/Cong/persistent/video/data'\n",
    "result_path = '/home/idies/workspace/Storage/Cong/persistent/video/result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters for ORB method\n",
    "threshold = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prior knowledge of the laptimes\n",
    "prior_time = 60\n",
    "\n",
    "## FPS, could be computed using function `get_FPS`\n",
    "fps = 29.93930396046777\n",
    "\n",
    "## Dataframe from function `video_to_image2` in video2image.ipynb\n",
    "df = pd.read_csv(os.path.join(data_path, 'train091108_raw.csv'))\n",
    "\n",
    "## Parameters for the function `get_similar_image`\n",
    "initial_idx = 0\n",
    "start = initial_idx + np.round(fps*prior_time).astype(int)\n",
    "end = 3600\n",
    "\n",
    "method = 'pHash'\n",
    "ascending = True\n",
    "\n",
    "coords = [[(0, 600), (1920, 740)], [(300, 0), (1920, 740)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "similar_imgs = get_similar_image(df, method, ascending, initial_idx, start, end, coords)\n",
    "elapsedTime = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_index(df, n, method, ascending):\n",
    "    \"\"\"\n",
    "    A function to get the weighted index of the similar images\n",
    "    \"\"\"\n",
    "    \n",
    "    temp = df.copy()[:n]\n",
    "    if ascending:\n",
    "        temp['weight'] = (1/(temp[method]+1e-8)) / sum(1/(temp[method]+1e-8))\n",
    "    else:\n",
    "        temp['weight'] = temp[method] / sum(temp[method])\n",
    "    return sum(temp['img_index']*temp['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lap_time2(initial_idx, end, fps, topn, df, method, ascending, coords):\n",
    "    \"\"\"\n",
    "    A function to estimate the laptimes\n",
    "    \n",
    "    Most parameters are for function `get_similar_image`\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {}\n",
    "    start = initial_idx + np.round(fps*prior_time).astype(int)\n",
    "    \n",
    "    #img_idxs = np.arange(initial_idx, initial_idx+3).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+3, initial_idx+6).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+6, initial_idx+9).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+9, initial_idx+12).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+12, initial_idx+15).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+15, initial_idx+18).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+18, initial_idx+21).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+21, initial_idx+24).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+24, initial_idx+27).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+27, np.round(initial_idx+fps).astype(int)).tolist()\n",
    "    \n",
    "    img_idxs = np.arange(initial_idx, initial_idx+5).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+5, initial_idx+10).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+10, initial_idx+15).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+15, initial_idx+20).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+20, initial_idx+25).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+25, np.round(initial_idx+fps).astype(int)).tolist()\n",
    "    \n",
    "    #img_idxs = np.arange(initial_idx, initial_idx+10).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+10, initial_idx+20).tolist()\n",
    "    #img_idxs = np.arange(initial_idx+20, np.round(initial_idx+fps).astype(int)).tolist()\n",
    "    \n",
    "    #img_idxs = np.choice(np.arange(initial_idx, np.round(initial_idx+fps).astype(int)), 10).tolist()\n",
    "    \n",
    "    for idx in img_idxs:\n",
    "        #img = imread(df['img_path'][idx])\n",
    "        start_time = time.time()\n",
    "        similar_imgs = get_similar_image(df, method, ascending, idx, start, end, coords)\n",
    "        elapsedTime = time.time() - start_time\n",
    "        results_dict[idx] = [df['img_path'][idx], df['img_index'][idx], \n",
    "                             (np.mean(similar_imgs.img_index[similar_imgs[method]==(similar_imgs[method].iloc[0])])-df['img_index'][idx])/fps, \n",
    "                             (weighted_index(similar_imgs, topn, method, ascending)-df['img_index'][idx])/fps, \n",
    "                             elapsedTime, method]\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "    results_df.columns = ['img_path', 'img_index', 'lap_time', 'weighted_lap_time', 'elapsedTime', 'method']\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## An example\n",
    "df = pd.read_csv(os.path.join(data_path, 'train091108_raw.csv'))\n",
    "\n",
    "initial_idx = 21681\n",
    "end = 26146\n",
    "prior_time = 60\n",
    "\n",
    "fps = 29.93930396046777\n",
    "topn = 5\n",
    "\n",
    "method = 'pHash'\n",
    "ascending = True\n",
    "\n",
    "coords = [[(0, 600), (1920, 740)], [(300, 0), (1920, 740)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = lap_time2(initial_idx, end, fps, topn, df, method, ascending, coords)\n",
    "test['lap'] = 9\n",
    "test.to_csv(os.path.join(result_path, '091108', 'lap9_pHash_6.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24359\n",
      "26155\n",
      "28828\n"
     ]
    }
   ],
   "source": [
    "temp1 = pd.read_csv(os.path.join(result_path, '091108', 'lap9_pHash_1.csv'))\n",
    "temp2 = pd.read_csv(os.path.join(result_path, '091108', 'lap9_pHash_2.csv'))\n",
    "temp3 = pd.read_csv(os.path.join(result_path, '091108', 'lap9_pHash_3.csv'))\n",
    "temp4 = pd.read_csv(os.path.join(result_path, '091108', 'lap9_pHash_4.csv'))\n",
    "temp5 = pd.read_csv(os.path.join(result_path, '091108', 'lap9_pHash_5.csv'))\n",
    "temp6 = pd.read_csv(os.path.join(result_path, '091108', 'lap9_pHash_6.csv'))\n",
    "# temp7 = pd.read_csv(os.path.join(result_path, '091108', 'lap9_pHash_7.csv'))\n",
    "# temp8 = pd.read_csv(os.path.join(result_path, '091108', 'lap9_pHash_8.csv'))\n",
    "# temp9 = pd.read_csv(os.path.join(result_path, '091108', 'lap9_pHash_9.csv'))\n",
    "# temp10 = pd.read_csv(os.path.join(result_path, '091108', 'lap9_pHash_10.csv'))\n",
    "temp = temp1.append(temp2)\n",
    "temp = temp.append(temp3)\n",
    "temp = temp.append(temp4)\n",
    "temp = temp.append(temp5)\n",
    "temp = temp.append(temp6)\n",
    "# temp = temp.append(temp7)\n",
    "# temp = temp.append(temp8)\n",
    "# temp = temp.append(temp9)\n",
    "# temp = temp.append(temp10)\n",
    "\n",
    "# initial_idx\n",
    "print(int(temp['lap_time'].iloc[0] * fps) + initial_idx)\n",
    "\n",
    "# start\n",
    "print(int(temp['lap_time'].iloc[0] * fps) + initial_idx + np.round(fps*prior_time).astype(int))\n",
    "\n",
    "# end\n",
    "print(int(int(temp['lap_time'].iloc[0] * fps) + initial_idx + np.round(fps*prior_time).astype(int) + np.mean(temp['lap_time']) * fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv(os.path.join(result_path, '091108', 'lap9_pHash.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.54227315615418"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.read_csv('./result/091108/lap9_pHash.csv')\n",
    "np.median(temp['lap_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
